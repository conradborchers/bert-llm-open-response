{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "992ca7a8-9b19-4495-9c94-dae2309e2a1f",
   "metadata": {},
   "source": [
    "## Read in LLM Grades and Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9648124-b2fd-4255-89a9-faf250563a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = \"llm-scores\"\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "# Iterate through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Check if the file is an Excel file\n",
    "    if filename.endswith(\".xlsx\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        # Read the Excel file and append it to the list of DataFrames\n",
    "        df = pd.read_excel(file_path)\n",
    "        dataframes.append(df)\n",
    "\n",
    "# Concatenate all the DataFrames\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "df2 = df[['Transaction_Id', 'GPT_4_turbo_predict', 'GPT_4_turbo_explain', 'GPT_4o_explain', 'GPT_4o_predict']].copy()\n",
    "\n",
    "df2['GPT_4_turbo'] = df2['GPT_4_turbo_predict'].combine_first(df2['GPT_4_turbo_explain'])\n",
    "df2['GPT_4o'] = df2['GPT_4o_predict'].combine_first(df2['GPT_4o_explain'])\n",
    "\n",
    "# Drop the original predict and explain columns (optional)\n",
    "df3 = df2.drop(columns=['GPT_4_turbo_predict', 'GPT_4_turbo_explain', 'GPT_4o_predict', 'GPT_4o_explain'])\n",
    "\n",
    "gpt_4_turbo_dict = df3.set_index('Transaction_Id')['GPT_4_turbo'].to_dict()\n",
    "gpt_4o_dict = df3.set_index('Transaction_Id')['GPT_4o'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5481101f-efc3-4e88-b10b-d859788734a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df = pd.read_csv('All_Data_Advocacy_Lessons_LAK25 - All Data - Revised.csv', low_memory=False)\n",
    "df = df[df['Is Last Attempt'] == 1].copy()\n",
    "df = df[df['Open_response_score_human_truth'].map(lambda s: not pd.isna(s))].copy()\n",
    "df['Type'] = df['Problem Name'].map(lambda s: 'predicted' if 'What' in s or 'what would you say' in s else 'explained')\n",
    "df = df[['Transaction_Id', 'Level_Level2_corrected', 'Type', 'Level (Lesson)', 'Input', 'Open_response_score_human_truth']].copy()\n",
    "\n",
    "data = df.copy()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, cohen_kappa_score\n",
    "\n",
    "data['combination'] = data['Type'] + \" - \" + data['Level (Lesson)']\n",
    "unique_combinations = data['combination'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa30cba-fae3-498e-ac24-1b18d25bd048",
   "metadata": {},
   "source": [
    "## Descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc555d6d-eaf3-4864-9c47-439fc107d882",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Level (Lesson)'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c798b042-31e6-41de-a164-1cfbbe67b3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iraise = data[data['Level (Lesson)'].isin(['Helping Students Manage Inequity', 'Avoiding Unconscious Assumptions'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4754352c-15a2-40a2-a489-e35a13ca5afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iraise.groupby(['Level (Lesson)', 'Type', 'Open_response_score_human_truth']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178a78fa-4a23-46fc-a020-1c9a809a5be7",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f9c835-4a8e-4772-85f6-66d02bb759ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['combination'] = data['Type'] + \" - \" + data['Level (Lesson)']\n",
    "unique_combinations = data['combination'].unique()\n",
    "\n",
    "# Define a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_and_evaluate(data, tokenizer, max_len, batch_size, epochs, max_splits=5):\n",
    "    results = []\n",
    "    aggregated_results = []\n",
    "\n",
    "    for comb in tqdm(unique_combinations):\n",
    "        print(f\"Processing combination: {comb}\")\n",
    "        comb_data = data[data['combination'] == comb].copy()\n",
    "        texts = comb_data['Input'].tolist()\n",
    "        labels = comb_data['Open_response_score_human_truth'].astype(int).tolist()\n",
    "\n",
    "        orig_texts = texts.copy()  # Needs deep copy as skf split overwrites objects during iteration\n",
    "        orig_labels = labels.copy()\n",
    "\n",
    "        # Debugging: Ensure lengths match\n",
    "        if len(texts) != len(labels):\n",
    "            print(f\"Skipping combination {comb}: Mismatched lengths - texts: {len(texts)}, labels: {len(labels)}\")\n",
    "            continue\n",
    "\n",
    "        # Skip combinations with fewer than 2 examples\n",
    "        if len(labels) < 2:\n",
    "            print(f\"Skipping combination {comb} (too few examples).\")\n",
    "            continue\n",
    "\n",
    "        # Adjust `n_splits` based on data size\n",
    "        n_splits = min(max_splits, len(labels))\n",
    "        skf = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "        fold_metrics = []  # Store metrics for each fold\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(texts, labels)):\n",
    "            if len(orig_texts) != len(orig_labels):\n",
    "                print(f\"Skipping combination {comb} in fold {fold}: Mismatched lengths - texts: {len(orig_texts)}, labels: {len(orig_labels)}\")\n",
    "                continue\n",
    "            print(f\"Fold {fold+1}/{n_splits}\")\n",
    "\n",
    "            train_texts = [orig_texts[i] for i in train_idx]\n",
    "            val_texts = [orig_texts[i] for i in val_idx]\n",
    "            train_labels = [orig_labels[i] for i in train_idx]\n",
    "            val_labels = [orig_labels[i] for i in val_idx]\n",
    "\n",
    "            train_dataset = CustomDataset(train_texts, train_labels, tokenizer, max_len)\n",
    "            val_dataset = CustomDataset(val_texts, val_labels, tokenizer, max_len)\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            model = BertForSequenceClassification.from_pretrained(\n",
    "                'bert-base-uncased', num_labels=2)\n",
    "            model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "            optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                model.train()\n",
    "                total_loss = 0\n",
    "                for batch in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    input_ids = batch['input_ids'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                    attention_mask = batch['attention_mask'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                    labels = batch['label'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "                    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "                    total_loss += loss.item()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                print(f\"Epoch {epoch+1} Loss: {total_loss/len(train_loader)}\")\n",
    "\n",
    "            # Evaluate\n",
    "            model.eval()\n",
    "            all_preds = []\n",
    "            all_probs = []\n",
    "            all_labels = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    input_ids = batch['input_ids'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                    attention_mask = batch['attention_mask'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                    labels = batch['label'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "                    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                    probs = torch.softmax(outputs.logits, axis=1)[:, 1]  # Probability of class 1\n",
    "                    preds = torch.argmax(outputs.logits, axis=1)\n",
    "\n",
    "                    all_probs.extend(probs.cpu().numpy())\n",
    "                    all_preds.extend(preds.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Calculate metrics\n",
    "            acc = accuracy_score(all_labels, all_preds)\n",
    "            f1 = f1_score(all_labels, all_preds)\n",
    "            auc = roc_auc_score(all_labels, all_probs) if len(set(all_labels)) > 1 else None\n",
    "            kappa = cohen_kappa_score(all_labels, all_preds)\n",
    "            report = classification_report(all_labels, all_preds, output_dict=True)\n",
    "\n",
    "            print(f\"Fold {fold+1} Metrics - Accuracy: {acc:.4f}, F1: {f1:.4f}, AUC: {auc}, Kappa: {kappa:.4f}\")\n",
    "\n",
    "            fold_metrics.append({\n",
    "                'fold': fold+1,\n",
    "                'accuracy': acc,\n",
    "                'f1': f1,\n",
    "                'auc': auc,\n",
    "                'kappa': kappa,\n",
    "                'report': report\n",
    "            })\n",
    "            results.append({\n",
    "                'combination': comb,\n",
    "                'fold': fold+1,\n",
    "                'accuracy': acc,\n",
    "                'f1': f1,\n",
    "                'auc': auc,\n",
    "                'kappa': kappa,\n",
    "                'report': report\n",
    "            })\n",
    "\n",
    "        # Calculate average metrics across folds for this combination\n",
    "        if fold_metrics:\n",
    "            avg_metrics = {\n",
    "                'combination': comb,\n",
    "                'accuracy': np.mean([fm['accuracy'] for fm in fold_metrics]),\n",
    "                'f1': np.mean([fm['f1'] for fm in fold_metrics]),\n",
    "                'auc': np.mean([fm['auc'] for fm in fold_metrics if fm['auc'] is not None]),\n",
    "                'kappa': np.mean([fm['kappa'] for fm in fold_metrics])\n",
    "            }\n",
    "            print(f\"Average Metrics for combination {comb}: {avg_metrics}\")\n",
    "            aggregated_results.append(avg_metrics)\n",
    "\n",
    "    return {'fold_results': results, 'avg_results': aggregated_results}\n",
    "\n",
    "# Parameters\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_len = 256\n",
    "batch_size = 16\n",
    "epochs = 10\n",
    "n_cv_splits = 5\n",
    "\n",
    "# Run training and evaluation\n",
    "results = train_and_evaluate(data, tokenizer, max_len, batch_size, epochs, max_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca276d87-8189-460a-a56e-520f09692dff",
   "metadata": {},
   "source": [
    "## Comparative BERT vs. LLM Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d15dbe7-0355-4903-95e6-b7d837365280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def evaluate_predictions(true_labels, pred_labels, model_name):\n",
    "    acc = accuracy_score(true_labels, pred_labels)\n",
    "    f1 = f1_score(true_labels, pred_labels, average='weighted')\n",
    "    auc = 0.0000 # undefined for llm \n",
    "    kappa = cohen_kappa_score(true_labels, pred_labels)\n",
    "    print(f\"{model_name} - Accuracy: {acc:.4f}, F1: {f1:.4f}, AUC: {auc}, Kappa: {kappa:.4f}\")\n",
    "    return {'accuracy': acc, 'f1': f1, 'auc': auc, 'kappa': kappa}\n",
    "    \n",
    "def train_and_evaluate_with_dicts(data, tokenizer, max_len, batch_size, epochs, max_splits=5):\n",
    "    results = []\n",
    "    for comb in tqdm(unique_combinations):\n",
    "        aggregated_results = []\n",
    "        bert_metrics_all_folds = []\n",
    "        gpt4_turbo_metrics_all_folds = []\n",
    "        gpt4o_metrics_all_folds = []\n",
    "        total_validation_size = 0  # Track total validation size across all folds\n",
    "        \n",
    "        print(f\"Processing combination: {comb}\")\n",
    "        comb_data = data[data['combination'] == comb].copy()\n",
    "        texts = comb_data['Input'].tolist()\n",
    "        labels = comb_data['Open_response_score_human_truth'].astype(int).tolist()\n",
    "\n",
    "        # Skip combinations with fewer than 2 labels (AUC undefined)\n",
    "        if len(labels) < 2:\n",
    "            print(f\"Skipping combination {comb} (too few labels).\")\n",
    "            continue\n",
    "\n",
    "        # Check for missing dictionary values\n",
    "        missing_turbo = sum(pd.isnull(comb_data['Transaction_Id'].map(gpt_4_turbo_dict)))\n",
    "        missing_gpt4o = sum(pd.isnull(comb_data['Transaction_Id'].map(gpt_4o_dict)))\n",
    "        print(f\"{comb} - Missing GPT_4_turbo: {missing_turbo}, Missing GPT_4o: {missing_gpt4o}\")\n",
    "\n",
    "        # Exclude rows with missing dictionary values\n",
    "        comb_data = comb_data[\n",
    "            ~comb_data['Transaction_Id'].map(gpt_4_turbo_dict).isnull()\n",
    "            & ~comb_data['Transaction_Id'].map(gpt_4o_dict).isnull()\n",
    "        ]\n",
    "\n",
    "        if comb_data.empty:\n",
    "            print(f\"Skipping combination {comb} after filtering missing dictionary values.\")\n",
    "            continue\n",
    "\n",
    "        # Add dictionary predictions\n",
    "        comb_data['GPT_4_turbo_eval'] = comb_data['Transaction_Id'].map(gpt_4_turbo_dict)\n",
    "        comb_data['GPT_4o_eval'] = comb_data['Transaction_Id'].map(gpt_4o_dict)\n",
    "\n",
    "        texts = comb_data['Input'].tolist()\n",
    "        labels = comb_data['Open_response_score_human_truth'].tolist()\n",
    "        turbo_preds = comb_data['GPT_4_turbo_eval'].tolist()\n",
    "        gpt4o_preds = comb_data['GPT_4o_eval'].tolist()\n",
    "\n",
    "        texts_orig = texts.copy()\n",
    "        labels_orig = labels.copy()\n",
    "        turbo_preds_orig = turbo_preds.copy()\n",
    "        gpt4o_preds_orig = gpt4o_preds.copy()\n",
    "\n",
    "        # Adjust `n_splits` based on data size\n",
    "        n_splits = min(max_splits, len(labels))\n",
    "        skf = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "        fold_metrics = []  # Store metrics for each fold\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(texts, labels)):\n",
    "            print(f\"Fold {fold+1}/{n_splits}\")\n",
    "\n",
    "            val_size = len(val_idx)  # Get size of validation set\n",
    "            total_validation_size += val_size\n",
    "            print(f\"Validation set size for Fold {fold+1}: {val_size}\")\n",
    "\n",
    "            train_texts = [texts_orig[i] for i in train_idx]\n",
    "            val_texts = [texts_orig[i] for i in val_idx]\n",
    "            train_labels = [labels_orig[i] for i in train_idx]\n",
    "            val_labels = [labels_orig[i] for i in val_idx]\n",
    "\n",
    "            # Evaluate dictionary predictions on validation data\n",
    "            val_turbo_preds = [turbo_preds_orig[i] for i in val_idx]\n",
    "            val_gpt4o_preds = [gpt4o_preds_orig[i] for i in val_idx]\n",
    "\n",
    "            turbo_metrics = evaluate_predictions(val_labels, val_turbo_preds, f\"GPT_4_turbo Fold {fold+1}\")\n",
    "            gpt4o_metrics = evaluate_predictions(val_labels, val_gpt4o_preds, f\"GPT_4o Fold {fold+1}\")\n",
    "\n",
    "            gpt4_turbo_metrics_all_folds.append(turbo_metrics)\n",
    "            gpt4o_metrics_all_folds.append(gpt4o_metrics)\n",
    "\n",
    "            aggregated_results.append({'combination': f\"{comb} - GPT_4_turbo Fold {fold+1}\", **turbo_metrics})\n",
    "            aggregated_results.append({'combination': f\"{comb} - GPT_4o Fold {fold+1}\", **gpt4o_metrics})\n",
    "\n",
    "            train_dataset = CustomDataset(train_texts, train_labels, tokenizer, max_len)\n",
    "            val_dataset = CustomDataset(val_texts, val_labels, tokenizer, max_len)\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            model = BertForSequenceClassification.from_pretrained(\n",
    "                'bert-base-uncased', num_labels=2)\n",
    "            model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "            optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                model.train()\n",
    "                total_loss = 0\n",
    "                for batch in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    input_ids = batch['input_ids'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                    attention_mask = batch['attention_mask'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                    labels = batch['label'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "                    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "                    total_loss += loss.item()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                print(f\"Epoch {epoch+1} Loss: {total_loss/len(train_loader)}\")\n",
    "\n",
    "            # Evaluate\n",
    "            model.eval()\n",
    "            all_preds = []\n",
    "            all_probs = []\n",
    "            all_labels = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    input_ids = batch['input_ids'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                    attention_mask = batch['attention_mask'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                    labels = batch['label'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "                    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                    probs = torch.softmax(outputs.logits, axis=1)[:, 1]  # Probability of class 1\n",
    "                    preds = torch.argmax(outputs.logits, axis=1)\n",
    "\n",
    "                    all_probs.extend(probs.cpu().numpy())\n",
    "                    all_preds.extend(preds.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Calculate metrics\n",
    "            acc = accuracy_score(all_labels, all_preds)\n",
    "            f1 = f1_score(all_labels, all_preds)\n",
    "            auc = roc_auc_score(all_labels, all_probs) if len(set(all_labels)) > 1 else None\n",
    "            kappa = cohen_kappa_score(all_labels, all_preds)\n",
    "\n",
    "            print(f\"Fold {fold+1} Metrics - Accuracy: {acc:.4f}, F1: {f1:.4f}, AUC: {auc}, Kappa: {kappa:.4f}\")\n",
    "\n",
    "            bert_metrics_all_folds.append({\n",
    "                'accuracy': acc,\n",
    "                'f1': f1,\n",
    "                'auc': auc,\n",
    "                'kappa': kappa,\n",
    "            })\n",
    "\n",
    "            fold_metrics.append({\n",
    "                'fold': fold+1,\n",
    "                'accuracy': acc,\n",
    "                'f1': f1,\n",
    "                'auc': auc,\n",
    "                'kappa': kappa,\n",
    "            })\n",
    "            results.append({\n",
    "                'combination': comb,\n",
    "                'fold': fold+1,\n",
    "                'accuracy': acc,\n",
    "                'f1': f1,\n",
    "                'auc': auc,\n",
    "                'kappa': kappa,\n",
    "            })\n",
    "\n",
    "        # Calculate average metrics across folds for this combination\n",
    "        if fold_metrics:\n",
    "            avg_metrics = {\n",
    "                'combination': comb,\n",
    "                'accuracy': np.mean([fm['accuracy'] for fm in fold_metrics]),\n",
    "                'f1': np.mean([fm['f1'] for fm in fold_metrics]),\n",
    "                'auc': np.mean([fm['auc'] for fm in fold_metrics if fm['auc'] is not None]),\n",
    "                'kappa': np.mean([fm['kappa'] for fm in fold_metrics])\n",
    "            }\n",
    "            print(f\"Average Metrics for combination {comb}: {avg_metrics}\")\n",
    "            aggregated_results.append(avg_metrics)\n",
    "        \n",
    "        # Aggregate metrics for BERT, GPT-4 Turbo, and GPT-4o\n",
    "        bert_avg = {key: np.mean([m[key] for m in bert_metrics_all_folds]) for key in bert_metrics_all_folds[0]}\n",
    "        gpt4_turbo_avg = {key: np.mean([m[key] for m in gpt4_turbo_metrics_all_folds]) for key in gpt4_turbo_metrics_all_folds[0]}\n",
    "        gpt4o_avg = {key: np.mean([m[key] for m in gpt4o_metrics_all_folds]) for key in gpt4o_metrics_all_folds[0]}\n",
    "    \n",
    "        print(f\"\\nFinal Metrics for {comb}:\")\n",
    "        print(f\"Total Validation Set Size: {total_validation_size}\")\n",
    "        print(f\"BERT Average Metrics: {bert_avg}\")\n",
    "        print(f\"GPT-4 Turbo Average Metrics: {gpt4_turbo_avg}\")\n",
    "        print(f\"GPT-4o Average Metrics: {gpt4o_avg}\")\n",
    "\n",
    "        results.append((comb, bert_avg, gpt4_turbo_avg, gpt4o_avg))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Parameters\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_len = 256\n",
    "batch_size = 16\n",
    "epochs = 5\n",
    "n_cv_splits = 5\n",
    "\n",
    "# Run training and evaluation\n",
    "results = train_and_evaluate_with_dicts(data, tokenizer, max_len, batch_size, epochs, max_splits=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
