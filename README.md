# Overview

This repository contains the datasets, grading rubrics, and code used in the study **"Picking the Right Tool for the Job: A Performance Comparison of Language Models for Open-Response Assessment"**.  
The study evaluates the performance of various language models, including BERT and GPT-based models, in assessing open-ended tutor responses within scenario-based advocacy training lessons.

## **Repository Contents**
- **Lesson Content**: Includes lesson content for the six advocacy lessons used.  
- **Human Annotation Rubrics**: Contains criteria for human evaluation of open responses.  
- **Generative AI Model Prompts**: Prompts used for open-response evaluation with Generative AI.  
- **`Code for Bert.ipynb`**: Script for fine-tuning and evaluating the BERT model.  

## **Dataset**
To access the lesson log data files, go to DataShop: [link](https://pslcdatashop.web.cmu.edu/DatasetInfo?datasetId=6250)
